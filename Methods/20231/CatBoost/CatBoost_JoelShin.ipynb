{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author information\n",
    "- Name: Joel Shin\n",
    "- email address: joel@handong.ac.kr\n",
    "- GitHub: https://github.com/JoellikeCoffee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.Introduction of the Catboost model\n",
    "CatBoost, developed by Yandex, signifies a significant advancement in machine learning algorithms, particularly within the domain of gradient boosting models. The moniker \"CatBoost\" is a fusion of the terms \"Categorical\" and \"Boosting,\" underscoring the model's core competencies: robustness and efficiency in handling categorical data.\n",
    "\n",
    "Traditionally, gradient boosting models have been primarily tailored for numerical data, necessitating an initial step of converting categorical data into numerical form to accommodate the model. This indispensable conversion process, unfortunately, often leads to substantial information loss, resulting in subpar model performance. To rectify this widespread issue, CatBoost was designed with the unique ability to process categorical data efficiently, without undermining the integrity of the information.\n",
    "\n",
    "Demonstrating operational versatility, CatBoost extends its functionality to both CPU and GPU platforms. Furthermore, it's renowned as one of the most efficient machine learning algorithms, outstripping competitors such as XGBoost and LightGBM in terms of algorithm training speed.\n",
    "\n",
    "Substantially contributing to the enhancement of machine learning algorithms dealing with categorical features, CatBoost, through its unique algorithm and emphasis on categorical data, has not only propelled the field of gradient boosting forward but also significantly broadened the horizons of machine learning as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. Key concept of Catboost\n",
    "The Catboost algorithm has five phases of operation.\n",
    "\n",
    "1. **Data Preparation**: CatBoost begins with a critical process, employing a distinctive encoding technique called \"Ordered Target Statistics\". It numerically transforms each categorical feature based on its sequence of occurrence, significantly enhancing the efficiency in managing categorical data.\n",
    "\n",
    "\n",
    "2. **Model Initialization**: Similar to traditional gradient boosting algorithms, CatBoost initiates with a rudimentary model, often a decision tree. This model endeavors to learn the optimal representation of the data, yielding initial predictions.\n",
    "\n",
    "\n",
    "3. **Residual Computation**: The precision of the initial model's predictions is gauged against the actual values. The discrepancy, known as the residual, is calculated between the predicted and actual values, indicating the accuracy of the preliminary predictions.\n",
    "\n",
    "\n",
    "4. **New Model Training**: Once the residuals are calculated, a new model is trained to rectify the inaccuracies identified by the preceding model. In this stage, CatBoost implements a unique technique, \"Ordered Boosting\", to forestall overfitting.\n",
    "\n",
    "\n",
    "5. **Model Combination**: The newly trained model is fused with the existing model to augment the accuracy of the overall model. This iterative process is executed until a comprehensive final model emerges. This final model, an ensemble of multiple models, leverages the strengths of each individual model to offer effective data predictions.\n",
    "\n",
    "A fundamental aspect of CatBoost's approach is its strategy of computing statistics for each category during the training process and employing these statistics for tree construction. CatBoost calculates target value statistics for each category, such as the count of samples in each category and the average target value for each category.\n",
    "\n",
    "The hallmark of CatBoost's methodology is the way it handles categorical features in gradient boosting, eliminating the need for preprocessing. This is achieved by computing and employing statistics for each category during the training phase to guide tree construction. The algorithm calculates the target value statistics for each category, such as the number of samples in each category and the mean target value for each category, allowing the algorithm to optimize the use of categorical features.\n",
    "The target value statistics for each category are computed using the following equation:\n",
    "\n",
    "$$ S_i = \\frac{\\sum_{j=1}^{n} y_j \\cdot [x_j = i] + \\alpha}{\\sum_{j=1}^{n} [x_j = i] + \\alpha} $$\n",
    "\n",
    "where $S_i$ is the target value statistic for category $i$, $y_j$ is the target value for sample $j$, $x_j$ is the categorical feature value for sample $j$, $n$ is the total number of samples, and $\\alpha$ is a smoothing parameter. This formula computes the average target value for each category, with the smoothing parameter $\\alpha$ preventing overfitting to categories with fewer occurrences.\n",
    "\n",
    "Additionally, CatBoost applies an innovative schema for calculating leaf values during tree structure selection, which contributes to reducing overfitting. This scheme assigns smaller weights to deeper trees and larger weights to shallower trees, curtailing the model's tendency to overfit to the training data.\n",
    "\n",
    "The primary strength of CatBoost is its proficiency in handling categorical features without requiring preprocessing. This characteristic makes the algorithm more efficient and user-friendly by eliminating the need for manual feature engineering. Moreover, CatBoost's method of calculating leaf values aids in avoiding overfitting, resulting in more precise predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3. Example\n",
    "For experiments, Titanic data freely available on kaggle used for modeling. After performing only basic character preprocessing, Catboost, Xgboost, and Lightgbm totaly three model compared with the model performance and training time. Derivative variable task to add categorical variables during preprocessing added In this prosess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set library\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the Titanic Dataset\n",
    "titanic_data = pd.read_csv('titanic.csv')\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = titanic_data.drop(['Survived'], axis=1)\n",
    "y = titanic_data['Survived']\n",
    "X.columns = X.columns.str.replace('[^\\w\\s-]', '')\n",
    "\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "X['Title'] = X['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "X['Title'] = X['Title'].replace(['Lady', 'Countess', 'Don', 'Sir', 'Jonkheer', 'Dona'], 'Noble')\n",
    "X['Title'] = X['Title'].replace(['Capt', 'Col', 'Dr', 'Major', 'Rev'], 'Officer')\n",
    "X['Title'] = X['Title'].replace('Mlle', 'Miss')\n",
    "X['Title'] = X['Title'].replace('Ms', 'Miss')\n",
    "X['Title'] = X['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "# Create a new feature FamilySize\n",
    "X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n",
    "\n",
    "# Create a new feature IsAlone\n",
    "X['IsAlone'] = 0\n",
    "X.loc[X['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "# Create a new feature CabinClass\n",
    "X['CabinClass'] = X['Cabin'].apply(lambda cabin: cabin[0] if type(cabin) is str else 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost: Computing time - 0.1280 sec, Accuracy - 0.8603\n",
      "XGBoost: Computing time - 0.0504 sec, Accuracy - 0.8268\n",
      "LightGBM: Computing time - 0.0760 sec, Accuracy - 0.8324\n"
     ]
    }
   ],
   "source": [
    "# Label Encoding\n",
    "categorical_features = ['Sex', 'Cabin', 'Embarked', 'Ticket', 'Name', 'Title', 'CabinClass']\n",
    "encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    X[feature] = encoder.fit_transform(X[feature].astype(str))\n",
    "\n",
    "# Set Validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n",
    "\n",
    "# Training and Evaluation\n",
    "models = {\n",
    "    'CatBoost': CatBoostClassifier(iterations=100, learning_rate=0.1, verbose=0),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, verbosity=0),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, learning_rate=0.1, verbosity=-1)\n",
    "}\n",
    "times = []\n",
    "accuracies = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    times.append(elapsed_time)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "# print result\n",
    "for i, name in enumerate(models.keys()):\n",
    "    print(f'{name}: Computing time - {times[i]:.4f} sec, Accuracy - {accuracies[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"Evaluation_of_Catboost.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "    <figcaption>Modeling learning times from official Catboost</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the experiments conducted with CPU, CatBoost exhibited the highest accuracy among the three models, achieving an evaluation score of 86%. Moreover, it also required the shortest training time. According to existing literature on CatBoost, this model is likely to demonstrate improved effectiveness when applied to larger datasets or those with a greater number of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4. Appendix\n",
    "- Catboost Official Website : https://catboost.ai/\n",
    "\n",
    "\n",
    "- Dataset : https://www.kaggle.com/c/titanic\n",
    "\n",
    "\n",
    "- Papers\n",
    "    - Dorogush, A. V., Ershov, V., & Gulin, A. (2018). CatBoost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363.\n",
    "    - Prokhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., & Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. Advances in neural information processing systems, 31.\n",
    "    \n",
    "    \n",
    "- Concept of Boosting Algorithm\n",
    "    - https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting\n",
    "    - https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
    "    - https://towardsdatascience.com/tagged/boosting\n",
    "    \n",
    "    \n",
    "- Accuracy Formula    \n",
    "$$\n",
    "\\text{{Accuracy}} = \\frac{{\\text{{True Positives}} + \\text{{True Negatives}}}}{{\\text{{True Positives}} + \\text{{False Positives}} + \\text{{True Negatives}} + \\text{{False Negatives}}}}\n",
    "$$\n",
    "\n",
    "\n",
    "- System Environment\n",
    "| System Environment| Information                        |\n",
    "|-------------------|------------------------------------|\n",
    "| OS                | Windows 11 Pro 21H2 Version        |\n",
    "| CPU               | 12th Gen Intel(R) Core(TM) i9-12900|\n",
    "| GPU               | NVIDIA GeForce RTX 3090 X 2EA      |\n",
    "| RAM               | Samsung 32GB DDR4 25600 X 4EA      |\n",
    "| Python Version    | 3.9.13                             |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d288b0b05eff9a885f46bca58a1e22eaa20d9eeebab83492fa79239d8adcb3ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
